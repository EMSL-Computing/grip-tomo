{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial notebook for GRIP-Tomo\n",
    "**August George, PNNL, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bd2f3",
   "metadata": {},
   "source": [
    "A quick tutorial on how to convert a pdb into graph, convert a density (.mrc) into a graph, and classify graphs\n",
    "\n",
    "For more in-depth information on each module, subroutine, and their inputs/outputs see the API reference in `/docs`\n",
    "\n",
    "All the data used here can be found in the `example_data` folder which contains a readme describing the files in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))  # add parent directory to path (/scripts) to import modules\n",
    "plt.rcParams['figure.figsize'] = [15, 10]  # make plots larger\n",
    "data_path = Path(Path(os.getcwd()).parent,'example_data')  # datafile path\n",
    "\n",
    "# import grip-tomo modules\n",
    "import pdb2graph as p2g\n",
    "import density2graph as d2g\n",
    "import graph2class as g2c\n",
    "\n",
    "print(\"modules imported  \\u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `pdb2graph.py`:  pdb --> graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e0c37",
   "metadata": {},
   "source": [
    "Converting an alpha helix bundle protein `3vjf.pdb` (found in `example_data`) into a graph, using an 8 Angstrom pairwise distance threshold for alpha carbons\n",
    "\n",
    "#### step 1: convert pdb to pandas dataframe using the `PDB_to_df()` subroutine \n",
    "\n",
    "Each alpha carbon (or atom) x,y,z coordinate is stored as well as residue ID and hydrophobicity\n",
    "\n",
    "`PDB_to_df()` requires the following inputs:\n",
    "- `pdb_code` is the name of protein (for output)\n",
    "- `fname` is the pdb file to turn into graph\n",
    "- `pdbx` is a flag to set if using .pdb (0) or .pdbx (1) file format\n",
    "- `o` is the residue indexing offest (default = 0). this can typically be kept = 0\n",
    "- `CA_only` is a flag to set if using only alpha carbons (1) or all atoms (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set input parameters\n",
    "pdb_code = '3vjf' # name of protein (for output)\n",
    "fname = Path(data_path,'3vjf.pdb')  # file to turn into graph\n",
    "pdbx = 0  # using .pdb (0) or .pdbx (1) file format\n",
    "o = 0  # residue indexing offest (default = 0)\n",
    "CA_only = 1  # only use alpha carbons for graph nodes\n",
    "\n",
    "### convert from pdb file into dataframe\n",
    "df = p2g.PDB_to_df(pdb_code, fname, pdbx, o)  # convert pdb file into dataframe of alpha carbon coordinates and s/n \n",
    "\n",
    "### display results\n",
    "print('\\ndataframe from PDB\\n')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: convert pandas dataframe into a networkx graph using the `PDB_df_to_G()` subroutine \n",
    "\n",
    "Graph nodes are assigned for each atom / alpha carbon x,y,z coordinate. The pairwise Euclidean distance between each atom / alpha carbon is calculated and pairs with a distance below a threshold are assigned edges. \n",
    "\n",
    "`PDB_df_to_G()` requires the following inputs:\n",
    "- `df` is the dataframe containing the x,y,z coordinates of the nodes\n",
    "- `d_cut` is the maximum Euclidean distance allowed between nodes to assign edges, in Angstroms\n",
    "\n",
    "use `d_cut` = 8 Angstroms as the distance cutoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set input parameter\n",
    "d_cut = 8 # pairwise distance cutoff for assigning edges, in Angstroms\n",
    "\n",
    "### convert from dataframe to graph\n",
    "G = p2g.PDB_df_to_G(df, d_cut)  # convert coordinate dataframe into network graph\n",
    "\n",
    "### display results\n",
    "plt.title('3vjf: pdb to graph')\n",
    "nx.draw(G, pos=nx.spring_layout(G), node_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3 [optional]: save the data as a graph xml file (.gexf) and datafile (.csv) using the `save_data()` subroutine \n",
    "\n",
    "`save_data()` requires the following inputs:\n",
    "- `df` is the dataframe containing the x,y,z coordinates of the nodes\n",
    "- `G` is the networkx graph\n",
    "- `df_outname` is the output filename for the dataframe\n",
    "- `G_outname` is the outputfilename for the graph\n",
    "\n",
    "_note: the data is saved in your current directory. to save in a specific directory use the `save_data_at_this_folder()` function which requires a filepath as the first input argument._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files (optional)\n",
    "# p2g.save_data(df, G, '3vjf_data', '3vjf_graph')  # save dataframe as .csv and graph as .gexf (current directory)\n",
    "# p2g.save_data_at_this_folder('your/path/goes/here', df, G, '3vjf_data', '3vjf_graph')  # save dataframe as .csv and graph as .gexf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `density2graph.py`: 3D density .mrc-->graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f13f5",
   "metadata": {},
   "source": [
    "Below is a an example using the 3VJF density file `3vjf_density.mrc`. \n",
    "\n",
    "#### step 1: load mrc file and normalize data using the `load_density_file()` and `normalize_and_threshold_data()` subroutines \n",
    "\n",
    "The density values are normalized and a threshold cutoff is applied to remove pixels with a density value below the threshold\n",
    "\n",
    "`load_density_file()` requires a `filename` as an input argument. \n",
    "\n",
    "`normalize_and_threshold_data()` requires the following input arguments:\n",
    "- `mrc` is  mrcfile data object (using the mrcfile python package)\n",
    "- `t` is the pixel intesity cutoff threshold\n",
    "- `noise_stdev` is the standard deviation of white Gaussian noise to add to the data. Default = 0 (no noise added)\n",
    "- `norm_T` is a Boolean flag that sets if the inputted threshold is a (raw) unnormalized value, or a normalized value (0,1). Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set input parameters\n",
    "fname =  Path(data_path,'3vjf_density.mrc')\n",
    "t = 0.5  # pixel intensity threshold - unnormalized\n",
    "\n",
    "### load density file, normalize the data and threshold, and extract the x,y,z coordinates of the remaining pixels\n",
    "mrc = d2g.load_density_file(fname)  # load density file\n",
    "xyz_data = d2g.normalize_and_threshold_data(mrc,t)  # normalize data and threshold and then apply threshold\n",
    "\n",
    "### display results\n",
    "print('x,y,z coordinates for high intensity pixels in density (.mrc)\\n')\n",
    "print(xyz_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: cluster data and get cluster centroids using the `cluster_data()` and `get_cluster_centroids()` subroutines \n",
    "\n",
    "The x,y,z data of the remaining density after thresholding is clustered using the DBSCAN algorithm in scikit-learn: [ref](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "\n",
    "Afterwords, the centroid coordinates of these clusters is calculated to be used as the graph nodes.\n",
    "\n",
    "`cluster_data()` requires the following input arguments:\n",
    "- `xyz_data` is an array of xyz coordinates from the density in the form of: A[0] = [x0,y0,z0]\n",
    "- `DBSCAN_epsilon` is the maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "- `DBSCAN_min_samples` is the number of samples (or total weight) in a neighborhood for a point to be considered as a core point\n",
    "\n",
    "`get_cluster centroids()` requires the following input arguements:\n",
    "- `xyz_data` is an array of xyz coordinates from the density in the form of: A[0] = [x0,y0,z0]\n",
    "- `model` is a sklearn DBSCAN clustering object \n",
    "\n",
    "`DBSCAN_epsilon` = 1, and `DBSCAN_min_samples`= 4 where found empirically to give good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set clustering parameters\n",
    "DBSCAN_epsilon = 1  # DBSCAN  epsilon\n",
    "DBSCAN_min_samples = 4  # DBSCAN min samples\n",
    "\n",
    "### perform clustering and get cluster centers\n",
    "model = d2g.cluster_data(xyz_data,DBSCAN_epsilon,DBSCAN_min_samples)  # cluster thresholded data using DBSCAN\n",
    "coarse_model = d2g.get_cluster_centroids(xyz_data,model)  # coarse grain model by getting cluster centroids\n",
    "\n",
    "### Plot the results\n",
    "print('clustering results:')\n",
    "fig = d2g.plot_clustering_results(xyz_data, coarse_model, figsize=10)  # plot results of clustering and coarse-graining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: create graph from cluster centroids using `create_and_save_graph()` subroutine\n",
    "\n",
    "Using each cluster centroid as a graph node, we assign edges between nodes if their pairwise Euclidean distance (in pixels) is not above a cutoff threshold.\n",
    "\n",
    "`create_and_save_graph()` requires the following input arguments:\n",
    "- `coarse_model` is an array of cluster centroids where A[0] = [centroid_x0, centroid_y0, centroid_z0]\n",
    "- `d_cut` is the pairwise distance cutoff for assigning edges to nodes, in pixels.\n",
    "- `out_fname` is the filename for output\n",
    "- `save` is a flag to save file (True) or not (False)\n",
    "\n",
    "Assuming we want an 8 Angstrom cutoff distance, and 1 voxel = 1 Angstrom, we use a cutoff distance of 8 voxels (d_cut=8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set input parameters\n",
    "d_cut = 8  # pairwise cutoff distance in voxels (edge is assigned to two nodes if Euclidean distance <= cutoff distance)\n",
    "\n",
    "### generate graph (use save=True to save a .gexf file)\n",
    "G = d2g.create_and_save_graph(coarse_model, d_cut, 'density2graph', save=False)  # create graph where nodes = centroids, edges assigned by pairwise cutoff\n",
    "\n",
    "### output results\n",
    "plt.title('3vjf density to graph:')\n",
    "nx.draw(G, pos=nx.spring_layout(G), node_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `graph2class.py`: graph --> class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff956b97",
   "metadata": {},
   "source": [
    "Comparing the similarity of graph network features from different protein classes\n",
    "\n",
    "The 'control' protein graph `3vjf_pdb2graph.gexf` was generated directly from the pdb structure `3vjf.pdb` using `pdb2graph.py`. \n",
    "\n",
    "The 'sample' protein graph `3vjf_density.gexf` was generated from a 3D volume density of 3VFJ and converted into a graph using `density2graph.py`.\n",
    "\n",
    "This approach can be extended to multiple 'control' protein graphs and 'sample' protein graphs\n",
    "\n",
    "For each graph, several network features are calculated:\n",
    "- 'n nodes',\n",
    "- 'n edges',\n",
    "- 'density',\n",
    "- 'diameter',\n",
    "- 'avg path length',\n",
    "- 'avg clustering',\n",
    "- 'max closeness centrality',\n",
    "- 'max eigenvector centrality',\n",
    "- 'max betweenness centrality',\n",
    "- 'degree assortativity',\n",
    "- 'max clique number',\n",
    "- 'n communities',\n",
    "\n",
    "#### step 1: set up which graph files and graph features to use for similarity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set input parameters\n",
    "class_files = ['3vjf_pdb2graph.gexf']\n",
    "sample_files = [\n",
    "        '3vjf_density2graph.gexf', \n",
    "        ]\n",
    "similarity_features_list = [\n",
    "            'n nodes',\n",
    "            'n edges',\n",
    "            'density',\n",
    "            'diameter',\n",
    "            'avg path length',\n",
    "            'avg clustering',\n",
    "            'max closeness centrality',\n",
    "            'max eigenvector centrality',\n",
    "            'max betweenness centrality',\n",
    "            'degree assortativity',\n",
    "            'max clique number',\n",
    "            'n communities',\n",
    "    ]\n",
    "\n",
    "### get file list for graph 'classes' and 'samples'\n",
    "class_list = [Path(data_path, fname) for fname in class_files]\n",
    "sample_list = [Path(data_path, fname) for fname in sample_files]\n",
    "\n",
    "### output results\n",
    "print('class graph files:\\n')\n",
    "print(class_list)\n",
    "print('\\nsample graph files:\\n')\n",
    "print(sample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: calculating graph similarity using the `classify_graphs()` subroutine\n",
    "\n",
    "For each class / sample graph pair, calculate (1 - relative distance) between each graph feature and then return the average across all features. \n",
    "\n",
    "`classify_graphs()` requires the following input arguments:\n",
    "- `class_list`  is a list of control/reference graph files (classes)\n",
    "- `sample_list` is a list of non-control/non-reference graph files (samples)\n",
    "- `feature_list` is a list of which features to use for similarity score and must be a valid key to the graph features dictionary/dataframe\n",
    "\n",
    "_note: you can change which subset of features to include in the scoring function by adjusting the `similarity_features_list` parameter._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classify based on network feature similarities\n",
    "classify_df = g2c.classify_graphs(class_list, sample_list, similarity_features_list)\n",
    "\n",
    "### display results\n",
    "print('similarity scores:')\n",
    "print(classify_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912a129",
   "metadata": {},
   "source": [
    "# next steps: review API documentation in `/docs` \n",
    "- plot all atom vs alpha carbon only using `plot_FA_and_CA_coordinates()` in `pdb2graph.py`\n",
    "- add Gaussian white noise to density data using `add_Gaussian_noise()` in `density2graph.py`\n",
    "- get `y_true` and `y_pred` for [scikit-learn classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) using `process_similarity_df()` in `graph2class.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('griptomo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1028cc6a5a98e7f38aef83413799ab6eee1f8c2818ad2c9418c7a08061d1ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
